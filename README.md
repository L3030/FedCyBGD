# FedCyBGD
Code for paper "Save It All: Enabling Full Parameter Tuning for Federated Large Language Models via Cycle Black Gradient Descent"

This project aims to explore the use of full-parameter tuning Language Models (LLMs) in the federated learning setting, with the objective of reducing communication, computation, and memory costs through the implementation of a cycle block update mechanism.
